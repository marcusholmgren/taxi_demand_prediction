import { SparkleIcon } from '@/components/SparkleIcon'

import { Layout } from '@/components/Layout'

export default function Page(props) {
    return (<Layout {...props} />)
}

---
![](@/images/hyperparameter-tuning.jpg)

## Commit v0.0.3 {{  date: '2023-12-08T19:44Z' }}

Training with two ML models to find the optimal framework for accuracy and training speed.

[XGBoost](https://xgboost.ai), an ensemble learning algorithm, stands out for its efficiency and accuracy. It builds upon decision trees, iteratively refining predictions and reducing errors.
[LightGBM](https://lightgbm.readthedocs.io/en/latest/), another ensemble technique, utilizes a leaf-wise growth strategy, resulting in faster training and lighter models compared to XGBoost.

However, the performance of these models can be further enhanced through feature engineering and hyperparameter tuning.
Feature engineering involves transforming and creating new features from existing data to capture relevant patterns and improve model interpretability.
Hyperparameter tuning, on the other hand, involves selecting the optimal values for the model's parameters to optimize its performance.

Hyperparameter tuning involves optimizing parameters like learning rate, regularization, and tree depth.
[Optuna](https://optuna.org) an open source hyperparameter optimization framework to automate hyperparameter search


---
![](@/images/visualize-training_data.jpg)

## Commit v0.0.2 {{  date: '2023-09-24T21:05Z' }}

### <SparkleIcon /> Visualize training data

Several new Jupyter notebooks were added to the project. They are steps for gathering, cleaning, and transforming the training and test data.
Lastly, the data is visualized to get a better understanding of what we're working with.

---
![](@/images/initial-commit.jpg)

## Commit v0.0.1 {{  date: '2023-09-15T22:05Z' }}

This is my journey through Pau Labarto Bajo's [Real-World ML](https://realworldmachinelearning.carrd.co) tutorial.

This is the initial commit that setup project structure and isolate the Python environment with conda. I'm not sure what this is going to be yet but I'm excited to find out.
